{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "g7kCGmQW6Kzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "os.environ[\"TORCH_USE_CUDA_DSA\"]   = \"1\"\n",
        "\n",
        "!pip install -q segmentation-models-pytorch==0.3.3\n",
        "!pip install -q albumentations==1.4.7 lpips torch-fidelity\n",
        "\n",
        "import json, random, cv2, numpy as np, matplotlib.pyplot as plt\n",
        "import shutil, zipfile\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import segmentation_models_pytorch as smp\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import albumentations as A\n",
        "import math\n",
        "from pathlib import Path\n",
        "from matplotlib import cm\n",
        "import re\n",
        "import gdown"
      ],
      "metadata": {
        "id": "lFANGADkEiPo"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = \"archive.zip\"\n",
        "\n",
        "if os.path.exists(zip_path):\n",
        "    os.remove(zip_path)\n",
        "\n",
        "gdown.download(id=\"1_HPn3NogV4NPD88i1Y9jDgUbr-dQKuHS\", output=zip_path, quiet=False)\n",
        "\n",
        "os.makedirs(\"datasets\", exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"datasets\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "for root, dirs, files in os.walk(\"datasets\"):\n",
        "    print(root,len(files))\n",
        "\n",
        "print(\"✅ Dataset downloaded succesfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MriS5gwx7nd6",
        "outputId": "58608bf1-f3bf-4be0-9189-221932cd44ab"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1_HPn3NogV4NPD88i1Y9jDgUbr-dQKuHS\n",
            "From (redirected): https://drive.google.com/uc?id=1_HPn3NogV4NPD88i1Y9jDgUbr-dQKuHS&confirm=t&uuid=05aa7fa0-adfb-4967-949a-591a516ac20e\n",
            "To: /content/archive.zip\n",
            "100%|██████████| 4.45G/4.45G [01:30<00:00, 49.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "datasets 0\n",
            "datasets/Teeth Segmentation PNG 1\n",
            "datasets/Teeth Segmentation PNG/d2 0\n",
            "datasets/Teeth Segmentation PNG/d2/masks_machine 598\n",
            "datasets/Teeth Segmentation PNG/d2/ann 598\n",
            "datasets/Teeth Segmentation PNG/d2/img 598\n",
            "datasets/Teeth Segmentation PNG/d2/masks_human 598\n",
            "datasets/Teeth Segmentation JSON 2\n",
            "datasets/Teeth Segmentation JSON/d2 0\n",
            "datasets/Teeth Segmentation JSON/d2/masks_machine 598\n",
            "datasets/Teeth Segmentation JSON/d2/ann 598\n",
            "datasets/Teeth Segmentation JSON/d2/predictions_transunet 126\n",
            "datasets/Teeth Segmentation JSON/d2/predictions_unet 136\n",
            "datasets/Teeth Segmentation JSON/d2/img 598\n",
            "datasets/Teeth Segmentation JSON/d2/masks_human 598\n",
            "✅ Dataset downloaded succesfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PASTE THE LINK COPIED IN THE PREVIOUS CODE HERE\n",
        "PUBLIC_URL = \"https://files.catbox.moe/v55u9l.zip\"\n",
        "\n",
        "TARGET_DIR = \"/content/synthetic_img\"\n",
        "ZIP_PATH   = \"/content/synthetic_img.zip\"\n",
        "\n",
        "!wget -O {ZIP_PATH} \"{PUBLIC_URL}\"\n",
        "\n",
        "if os.path.exists(TARGET_DIR):\n",
        "    shutil.rmtree(TARGET_DIR)\n",
        "os.makedirs(TARGET_DIR, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(ZIP_PATH, 'r') as zf:\n",
        "    zf.extractall(TARGET_DIR)\n",
        "\n",
        "print(\"✅ Dataset ready in:\", TARGET_DIR)\n",
        "!ls -lh {TARGET_DIR} | head -n 2"
      ],
      "metadata": {
        "id": "Ixy_nSLq7vEx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd984fb8-f806-4b4a-e18a-2e39647e44a9"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-08 15:30:50--  https://files.catbox.moe/v55u9l.zip\n",
            "Resolving files.catbox.moe (files.catbox.moe)... 108.181.20.35\n",
            "Connecting to files.catbox.moe (files.catbox.moe)|108.181.20.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9473966 (9.0M) [application/zip]\n",
            "Saving to: ‘/content/synthetic_img.zip’\n",
            "\n",
            "/content/synthetic_ 100%[===================>]   9.03M   730KB/s    in 17s     \n",
            "\n",
            "2025-09-08 15:31:08 (557 KB/s) - ‘/content/synthetic_img.zip’ saved [9473966/9473966]\n",
            "\n",
            "✅ Dataset ready in: /content/synthetic_img\n",
            "total 9.5M\n",
            "-rw-r--r-- 1 root root 47K Sep  8 15:31 102.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Globals"
      ],
      "metadata": {
        "id": "8HnbHIQ06lWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Editable parameters\n",
        "EPOCHS = 100\n",
        "IMG_SIZE = 256\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "DATA_MODE     = \"mix\"            # \"real\" | \"syn\" | \"mix\"\n",
        "MIX_STRATEGY  = \"replace\"        # \"concat\" | \"replace\" | \"ratio\"\n",
        "SYN_RATIO     = 0.3\n",
        "\n",
        "DATA_MODE_VAL = \"mix\"            # \"real\" | \"syn\" | \"mix\"\n",
        "MIX_STRATEGY_VAL = \"ratio\"       # \"concat\" | \"replace\" | \"ratio\""
      ],
      "metadata": {
        "id": "0NabbO9n7doy"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_MODE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "8PDYm5NiycHJ",
        "outputId": "2934a6f1-ff2a-49b4-99cf-05f573fa7075"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mix'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path dataset\n",
        "ROOT = \"/content/datasets/Teeth Segmentation JSON/d2\"\n",
        "IMG_DIR = os.path.join(ROOT, \"img\")\n",
        "HUM_DIR = os.path.join(ROOT, \"masks_human\")\n",
        "ANN_DIR = os.path.join(ROOT, \"masks_human\")\n",
        "META_PATH = \"/content/datasets/Teeth Segmentation JSON/meta.json\"\n",
        "MAP_PATH  = \"/content/datasets/Teeth Segmentation JSON/obj_class_to_machine_color.json\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Fixed parameters\n",
        "NUM_WORKERS = 2\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "IGNORE_INDEX = 255\n",
        "\n",
        "SYN_DIR = globals().get(\"TARGET_DIR\", None)\n",
        "ANN_DIR = globals().get(\"ANN_DIR\", None)\n",
        "\n",
        "NUM_CLASSES = 33  # 32 teeth + background\n",
        "BG_ID = 0\n",
        "\n",
        "IMG_EXTS = {\".png\",\".jpg\",\".jpeg\",\".bmp\",\".tif\",\".tiff\"}\n",
        "SUFFS = ['_img','-img','_image','-image','_mask','-mask','_seg','-seg','_label','-label']\n",
        "EXTS = {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\"}"
      ],
      "metadata": {
        "id": "tj9ywd8cEH9R"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "dQQxNYxn6xSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def list_files(d):\n",
        "    exts = (\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\")\n",
        "    return sorted([f for f in os.listdir(d) if f.lower().endswith(exts)])\n",
        "\n",
        "def to_map(dirpath):\n",
        "    return {os.path.splitext(f)[0]: os.path.join(dirpath, f) for f in list_files(dirpath)}\n",
        "\n",
        "\n",
        "def key_from_json_filename(fn: str) -> str:\n",
        "    # \"5.jpg.json\" -> \"5\"\n",
        "    base = os.path.basename(fn)\n",
        "    if base.lower().endswith(\".json\"):\n",
        "        base = base[:-5]\n",
        "    base, _ = os.path.splitext(base)\n",
        "    return base\n",
        "\n",
        "\n",
        "def safe_hex_to_rgb(h: str):\n",
        "    h = (h or \"#FFFFFF\").strip().lstrip(\"#\")\n",
        "    if len(h) == 3: h = \"\".join([c*2 for c in h])\n",
        "    h = h.ljust(6, \"0\")[:6]\n",
        "    return tuple(int(h[i:i+2], 16) for i in (0,2,4))\n",
        "\n",
        "\n",
        "def colorize_idx(idx_map: np.ndarray, ignore_index: int = IGNORE_INDEX) -> np.ndarray:\n",
        "    vis = idx_map.astype(np.int32).copy()\n",
        "    vis[vis == ignore_index] = 0\n",
        "    return palette[vis]\n",
        "\n",
        "\n",
        "def stems_set(d):\n",
        "    d = Path(d)\n",
        "    return {q.stem for q in d.iterdir() if q.is_file() and q.suffix.lower() in EXTS}\n",
        "\n",
        "def normk(s: str):\n",
        "    s = Path(s).stem.strip().lower()\n",
        "    for suf in SUFFS:\n",
        "        if s.endswith(suf):\n",
        "            s = s[:-len(suf)]\n",
        "    if s.isdigit():\n",
        "        s = str(int(s))\n",
        "    return s\n",
        "\n",
        "SUFFS = ['_img','-img','_image','-image','_mask','-mask','_seg','-seg','_label','-label']\n",
        "def norm(k: str):\n",
        "    s = Path(k).stem.strip().lower()\n",
        "    for suf in SUFFS:\n",
        "        if s.endswith(suf):\n",
        "            s = s[:-len(suf)]\n",
        "    if s.isdigit():\n",
        "        s = str(int(s))\n",
        "    return s\n",
        "\n",
        "def idx_norm(m):\n",
        "    out = {}\n",
        "    for k,p in m.items():\n",
        "        nk = norm(k)\n",
        "        if nk not in out: out[nk] = p\n",
        "    return out\n",
        "\n",
        "\n",
        "def norm_key(name: str):\n",
        "    s = Path(name).name.strip().lower()\n",
        "    while True:\n",
        "        root, ext = os.path.splitext(s)\n",
        "        if ext.lower() == \".json\" or ext.lower() in IMG_EXTS:\n",
        "            s = root\n",
        "            continue\n",
        "        break\n",
        "    for suf in SUFFS:\n",
        "        if s.endswith(suf): s = s[:-len(suf)]\n",
        "    if s.isdigit(): s = str(int(s))\n",
        "    return s\n",
        "\n",
        "\n",
        "# index JSON\n",
        "json_idx = {}\n",
        "for fn in os.listdir(ANN_DIR):\n",
        "    if fn.lower().endswith(\".json\"):\n",
        "        json_idx[norm_key(fn)] = os.path.join(ANN_DIR, fn)\n",
        "\n",
        "# find couples synth+json\n",
        "synth_paths = [p for p in Path(SYN_DIR).iterdir() if p.is_file() and p.suffix.lower() in IMG_EXTS]\n",
        "pairs = []\n",
        "for p in synth_paths:\n",
        "    nk = norm_key(p.stem)\n",
        "    jp = json_idx.get(nk)\n",
        "    if jp: pairs.append((str(p), jp, p.stem))\n",
        "\n",
        "def idx_color_bgr(i):\n",
        "    r,g,b,_ = cm.tab20(i % 20)\n",
        "    return (int(b*255), int(g*255), int(r*255))\n"
      ],
      "metadata": {
        "id": "I_MtSFs57tO-"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rasterize_any(json_path, out_shape_hw):\n",
        "    Ht, Wt = out_shape_hw\n",
        "    with open(json_path, \"r\") as f:\n",
        "        d = json.load(f)\n",
        "\n",
        "    # A) LabelMe\n",
        "    if \"shapes\" in d:\n",
        "        Hs = d.get(\"imageHeight\", Ht); Ws = d.get(\"imageWidth\", Wt)\n",
        "        sx, sy = Wt/float(Ws), Ht/float(Hs)\n",
        "        m = np.zeros((Ht, Wt), np.uint8)\n",
        "        for s in d[\"shapes\"]:\n",
        "            pts = s.get(\"points\", [])\n",
        "            if len(pts) >= 3:\n",
        "                poly = np.array([[int(round(x*sx)), int(round(y*sy))] for x,y in pts], np.int32)\n",
        "                cv2.fillPoly(m, [poly], 255)\n",
        "        return m\n",
        "\n",
        "    # B) Supervisely\n",
        "    objs = d.get(\"objects\") or d.get(\"labels\")\n",
        "    if objs is not None:\n",
        "        Hs = (d.get(\"size\", {}) or {}).get(\"height\") or d.get(\"imgHeight\") or Ht\n",
        "        Ws = (d.get(\"size\", {}) or {}).get(\"width\")  or d.get(\"imgWidth\")  or Wt\n",
        "        sx, sy = Wt/float(Ws), Ht/float(Hs)\n",
        "        m = np.zeros((Ht, Wt), np.uint8)\n",
        "        for obj in objs:\n",
        "            pts = None\n",
        "            if \"points\" in obj and \"exterior\" in obj[\"points\"]:\n",
        "                pts = obj[\"points\"][\"exterior\"]\n",
        "            elif \"geometry\" in obj and \"points\" in obj[\"geometry\"] and \"exterior\" in obj[\"geometry\"][\"points\"]:\n",
        "                pts = obj[\"geometry\"][\"points\"][\"exterior\"]\n",
        "            if not pts: continue\n",
        "            poly = np.array([[int(round(x*sx)), int(round(y*sy))] for x,y in pts], np.int32)\n",
        "            if poly.shape[0] >= 3:\n",
        "                cv2.fillPoly(m, [poly], 255)\n",
        "        return m\n",
        "    return np.zeros((Ht, Wt), np.uint8)\n",
        "\n",
        "def map_title_to_idx(title: str) -> int | None:\n",
        "    t = str(title).strip()\n",
        "    return TITLE2IDX.get(t, None)\n",
        "\n",
        "\n",
        "def rasterize_labels_33_meta(json_path: str, out_shape_hw: tuple[int,int]) -> np.ndarray:\n",
        "    H, W = out_shape_hw\n",
        "    with open(json_path, \"r\") as f:\n",
        "        d = json.load(f)\n",
        "\n",
        "    lab = np.full((H, W), BG_ID, np.uint8)\n",
        "\n",
        "    # LabelMe\n",
        "    if \"shapes\" in d:\n",
        "        Hs = d.get(\"imageHeight\", H); Ws = d.get(\"imageWidth\", W)\n",
        "        sx, sy = W/float(Ws), H/float(Hs)\n",
        "        for s in d[\"shapes\"]:\n",
        "            cid = map_title_to_idx(s.get(\"label\", \"\"))\n",
        "            if cid is None:\n",
        "                continue\n",
        "            pts = s.get(\"points\", [])\n",
        "            if len(pts) < 3:\n",
        "                continue\n",
        "            poly = np.array([[int(round(x*sx)), int(round(y*sy))] for x,y in pts], np.int32)\n",
        "            cv2.fillPoly(lab, [poly], int(cid))\n",
        "        return lab\n",
        "\n",
        "    # Supervisely\n",
        "    objs = d.get(\"objects\") or d.get(\"labels\")\n",
        "    if objs is not None:\n",
        "        Hs = (d.get(\"size\", {}) or {}).get(\"height\") or d.get(\"imgHeight\") or H\n",
        "        Ws = (d.get(\"size\", {}) or {}).get(\"width\")  or d.get(\"imgWidth\")  or W\n",
        "        sx, sy = W/float(Ws), H/float(Hs)\n",
        "        for obj in objs:\n",
        "            cid = map_title_to_idx(obj.get(\"classTitle\", \"\"))\n",
        "            if cid is None:\n",
        "                continue\n",
        "            # punti\n",
        "            pts = None\n",
        "            if \"points\" in obj and \"exterior\" in obj[\"points\"]:\n",
        "                pts = obj[\"points\"][\"exterior\"]\n",
        "            elif \"geometry\" in obj and \"points\" in obj[\"geometry\"] and \"exterior\" in obj[\"geometry\"][\"points\"]:\n",
        "                pts = obj[\"geometry\"][\"points\"][\"exterior\"]\n",
        "            if not pts:\n",
        "                continue\n",
        "            poly = np.array([[int(round(x*sx)), int(round(y*sy))] for x,y in pts], np.int32)\n",
        "            cv2.fillPoly(lab, [poly], int(cid))\n",
        "        return lab\n",
        "\n",
        "    return lab"
      ],
      "metadata": {
        "id": "cRrdjczr8amh"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "mMjVamO562YR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pair synth+json\n",
        "synth_paths = [p for p in Path(SYN_DIR).iterdir() if p.is_file() and p.suffix.lower() in IMG_EXTS]\n",
        "pairs = []\n",
        "for p in synth_paths:\n",
        "    nk = norm_key(p.stem)\n",
        "    jp = json_idx.get(nk)\n",
        "    if jp: pairs.append((str(p), jp, p.stem))\n",
        "print(f\"Couples found: {len(pairs)}/{len(synth_paths)} (SYN='{SYN_DIR}', ANN='{ANN_DIR}')\")\n",
        "\n",
        "# Meta & palette\n",
        "with open(META_PATH, \"r\") as f:\n",
        "    meta = json.load(f)\n",
        "\n",
        "# Take only the 32 teeth\n",
        "classes_raw = [c for c in meta[\"classes\"] if str(c.get(\"title\",\"\")).isdigit()]\n",
        "classes_sorted = sorted(classes_raw, key=lambda c: int(c[\"title\"]))\n",
        "\n",
        "CLASS_TITLES = [c[\"title\"] for c in classes_sorted]\n",
        "TITLE2IDX    = {t: i+1 for i,t in enumerate(CLASS_TITLES)}\n",
        "IDX2TITLE    = {v:k for k,v in TITLE2IDX.items()}\n",
        "N_CLASSES    = 1 + len(CLASS_TITLES)\n",
        "\n",
        "# Palette RGB to visualize (0=black)\n",
        "palette = np.zeros((N_CLASSES, 3), dtype=np.uint8)\n",
        "for c in classes_sorted:\n",
        "    idx = TITLE2IDX[c[\"title\"]]\n",
        "    palette[idx] = np.array(safe_hex_to_rgb(c.get(\"color\",\"#FFFFFF\")), dtype=np.uint8)\n"
      ],
      "metadata": {
        "id": "Usou1VhI9DiJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51e3f59a-cad9-45fc-b1b7-3bdc7c7c251f"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Couples found: 0/200 (SYN='/content/synthetic_img', ANN='/content/datasets/Teeth Segmentation JSON/d2/masks_human')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === RAW MAPS RECONSTRUCTION (drop-in, aderente al tuo notebook) ===\n",
        "# Usa: ROOT, IMG_DIR, TARGET_DIR (== cartella sintetici), ANN_DIR\n",
        "# e le funzioni già definite: list_files, to_map, norm, norm_key, idx_norm\n",
        "\n",
        "# 1) JSON: unica sorgente annotazioni (come in MIX)\n",
        "ANN_DIR = os.path.join(ROOT, \"ann\")  # <-- è così anche nel tuo file\n",
        "assert os.path.isdir(ANN_DIR), f\"ANN_DIR non esiste: {ANN_DIR}\"\n",
        "\n",
        "json_map_raw = {}\n",
        "for fn in os.listdir(ANN_DIR):\n",
        "    if fn.lower().endswith(\".json\"):\n",
        "        k = key_from_json_filename(fn)   # tua utility già definita\n",
        "        json_map_raw[k] = os.path.join(ANN_DIR, fn)\n",
        "\n",
        "# 2) Immagini reali e sintetiche\n",
        "assert os.path.isdir(IMG_DIR), f\"IMG_DIR non esiste: {IMG_DIR}\"\n",
        "img_map_raw  = to_map(IMG_DIR)\n",
        "\n",
        "# TARGET_DIR è già usato in notebook come cartella sintetici\n",
        "assert TARGET_DIR is not None, \"TARGET_DIR non impostata (cartella sintetici).\"\n",
        "assert os.path.isdir(TARGET_DIR), f\"TARGET_DIR non esiste: {TARGET_DIR}\"\n",
        "syn_map_raw  = to_map(TARGET_DIR)\n",
        "\n",
        "# 3) Normalizzazione chiavi (rimuove _img/_mask, estensioni, zeri iniziali, ecc.)\n",
        "img_idx  = idx_norm(img_map_raw)\n",
        "syn_idx  = idx_norm(syn_map_raw)\n",
        "\n",
        "# Costruiamo un indice coerente per i JSON usando norm_key (già tua)\n",
        "json_idx = {}\n",
        "for fn in os.listdir(ANN_DIR):\n",
        "    if fn.lower().endswith(\".json\"):\n",
        "        json_idx[norm_key(fn)] = os.path.join(ANN_DIR, fn)\n",
        "\n",
        "# 4) Intersezioni: REAL e SYN usano le stesse annotazioni (json_idx) per stesso stem\n",
        "real_keys = sorted(set(img_idx)  & set(json_idx))\n",
        "syn_keys  = sorted(set(syn_idx)  & set(json_idx))\n",
        "\n",
        "# 5) Mappe finali con le stesse chiavi\n",
        "img_map      = {k: img_idx[k]   for k in real_keys}     # real images\n",
        "json_map     = {k: json_idx[k]  for k in real_keys}     # real json (valide anche per syn)\n",
        "syn_img_map  = {k: syn_idx[k]   for k in syn_keys}      # synthetic images\n",
        "\n",
        "print(f\"Real usable (img∩json): {len(real_keys)}  | Synth usable (syn∩json): {len(syn_keys)}\")\n",
        "if len(syn_keys) == 0:\n",
        "    # Messaggio chiaro: spesso qui crolla il SYN mode quando gli stem non matchano\n",
        "    sample_syn = sorted(list(syn_idx.keys()))[:3]\n",
        "    sample_json = sorted(list(json_idx.keys()))[:3]\n",
        "    raise RuntimeError(\n",
        "        \"mode='syn' ma syn_img_map è vuoto.\\n\"\n",
        "        f\"- TARGET_DIR: {TARGET_DIR}\\n\"\n",
        "        f\"- Esempi stem SYN:  {sample_syn}\\n\"\n",
        "        f\"- Esempi stem JSON: {sample_json}\\n\"\n",
        "        \"Controlla suffix/prefix nei nomi: _img, -img, _image, -image, ecc. \"\n",
        "        \"La funzione norm()/norm_key() li rimuove: se hai nomi strani (es. '00123_res9'), \"\n",
        "        \"uniformali oppure estendi SUFFS.\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "bsp7u0YY9JeA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "209de8eb-688a-4a40-ecf8-682cf4a641b9"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Real usable (img∩json): 598  | Synth usable (syn∩json): 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network"
      ],
      "metadata": {
        "id": "_CJxK4pq61_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SYN_DIR  = \"/content/synthetic_img\"\n",
        "REAL_DIR = IMG_DIR\n",
        "\n",
        "# stem computation\n",
        "st_syn  = stems_set(SYN_DIR)\n",
        "st_real = stems_set(REAL_DIR)\n",
        "\n",
        "# intersection\n",
        "common = sorted(st_syn & st_real)\n",
        "\n",
        "print(f\"✔️ Found {len(common)} matched samples (img ↔ ann). Example: {common[0] if common else 'None'}\")\n",
        "\n",
        "train_tf = A.Compose([\n",
        "    A.Resize(IMG_SIZE, IMG_SIZE, interpolation=cv2.INTER_NEAREST),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.RandomRotate90(p=0.5),\n",
        "    ToTensorV2(transpose_mask=True)\n",
        "])\n",
        "\n",
        "val_tf = A.Compose([\n",
        "    A.Resize(IMG_SIZE, IMG_SIZE, interpolation=cv2.INTER_NEAREST),\n",
        "    ToTensorV2(transpose_mask=True)\n",
        "])"
      ],
      "metadata": {
        "id": "txVLddg18ulh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc5311de-c2e2-4a68-ca50-39e1d0d2b8ee"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔️ Found 200 matched samples (img ↔ ann). Example: 102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TeethSegDataset(Dataset):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        real_img_map: dict,\n",
        "        real_json_map: dict,\n",
        "        syn_img_map: dict = None,\n",
        "        syn_json_map: dict = None,\n",
        "        mode: str = \"real\",\n",
        "        mix_strategy: str = \"concat\",\n",
        "        syn_ratio: float = 0.5,\n",
        "        transform=None\n",
        "    ):\n",
        "        self.real_img_map = real_img_map or {}\n",
        "        self.real_json_map = real_json_map or {}\n",
        "        self.syn_img_map = syn_img_map or {}\n",
        "        self.syn_json_map = syn_json_map\n",
        "        self.mode = mode\n",
        "        self.mix_strategy = mix_strategy\n",
        "        self.syn_ratio = float(syn_ratio)\n",
        "        self.transform = transform\n",
        "        self.keys_real = list(self.real_img_map.keys())\n",
        "        self.keys_syn  = list(self.syn_img_map.keys())\n",
        "\n",
        "        if mode == \"real\":\n",
        "            self.index = [(\"real\", k) for k in self.keys_real]\n",
        "\n",
        "        elif mode == \"syn\":\n",
        "            if not self.keys_syn:\n",
        "                raise ValueError(\"mode='syn' ma syn_img_map è vuoto.\")\n",
        "            self.index = [(\"syn\", k) for k in self.keys_syn]\n",
        "\n",
        "        elif mode == \"mix\":\n",
        "            if not self.keys_syn:\n",
        "                raise ValueError(\"mode='mix' ma syn_img_map è vuoto.\")\n",
        "            if mix_strategy == \"concat\":\n",
        "                self.index = [(\"real\", k) for k in self.keys_real] + [(\"syn\", k) for k in self.keys_syn]\n",
        "\n",
        "            elif mix_strategy == \"replace\":\n",
        "                self.index = []\n",
        "                for k in self.keys_real:\n",
        "                    src = \"syn\" if k in self.syn_img_map else \"real\"\n",
        "                    self.index.append((src, k))\n",
        "\n",
        "            elif mix_strategy == \"ratio\":\n",
        "                self.L = max(len(self.keys_real), len(self.keys_syn))\n",
        "                self.index = None\n",
        "            else:\n",
        "                raise ValueError(\"mix_strategy has to be in {'concat','replace','ratio'}\")\n",
        "        else:\n",
        "            raise ValueError(\"mode has to be in {'real','syn','mix'}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.mode == \"mix\" and self.mix_strategy == \"ratio\":\n",
        "            return self.L\n",
        "        return len(self.index)\n",
        "\n",
        "    def _load_pair(self, src, k):\n",
        "        if src == \"real\":\n",
        "            img_p = self.real_img_map[k]\n",
        "            json_p = self.real_json_map[k]\n",
        "        else:\n",
        "            img_p = self.syn_img_map[k]\n",
        "            if self.syn_json_map and (k in self.syn_json_map):\n",
        "                json_p = self.syn_json_map[k]\n",
        "            else:\n",
        "                if k not in self.real_json_map:\n",
        "                    raise KeyError(f\"Missing target for the synthetic key '{k}'.\")\n",
        "                json_p = self.real_json_map[k]\n",
        "        img = cv2.imread(img_p, cv2.IMREAD_GRAYSCALE)\n",
        "        if img is None:\n",
        "            raise FileNotFoundError(img_p)\n",
        "        target = target = rasterize_labels_33_meta(json_p, img.shape)\n",
        "        return img, target\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.mode == \"mix\" and self.mix_strategy == \"ratio\":\n",
        "            use_syn = (random.random() < self.syn_ratio)\n",
        "            if use_syn:\n",
        "                k = random.choice(self.keys_syn)\n",
        "                src = \"syn\"\n",
        "            else:\n",
        "                k = random.choice(self.keys_real)\n",
        "                src = \"real\"\n",
        "        else:\n",
        "            src, k = self.index[idx]\n",
        "\n",
        "        img, target = self._load_pair(src, k)\n",
        "\n",
        "        # Albumentations\n",
        "        img_np = img[:, :, None]\n",
        "        if self.transform:\n",
        "            aug = self.transform(image=img_np, mask=target)\n",
        "            x = aug[\"image\"]\n",
        "            y = aug[\"mask\"].long()\n",
        "        else:\n",
        "            x = torch.from_numpy(img_np.transpose(2,0,1)).float() / 255.0\n",
        "            y = torch.from_numpy(target).long()\n",
        "\n",
        "        return x, y, k"
      ],
      "metadata": {
        "id": "Y-KdciMV9gyx"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "5Fx3NYgf63vn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(42)\n",
        "random.shuffle(common)\n",
        "val_ratio = 0.2\n",
        "val_count = max(1, int(len(common) * val_ratio))\n",
        "val_keys = sorted(common[:val_count])\n",
        "train_keys = [k for k in common if k not in val_keys]\n",
        "print(f\"Train: {len(train_keys)} | Val: {len(val_keys)}\")"
      ],
      "metadata": {
        "id": "aOQhV5tp8vGR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c33fe37d-4863-40d7-afff-f6f77cefd0b4"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 160 | Val: 40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_val     = max(1, int(round(len(real_keys) * val_ratio)))\n",
        "val_keys  = set(random.sample(real_keys, n_val))\n",
        "train_keys_real = [k for k in real_keys if k not in val_keys]\n",
        "\n",
        "train_keys_syn = [k for k in syn_keys if k in train_keys_real]\n",
        "val_keys_syn   = [k for k in syn_keys if k in val_keys]\n",
        "\n",
        "# safety: disjointed splits\n",
        "assert set(train_keys_real).isdisjoint(val_keys), \"Leak between train and val (REAL)!\"\n",
        "assert set(train_keys_syn).isdisjoint(set(val_keys_syn)), \"Leak between train and val (SYN)!\"\n",
        "\n",
        "real_img_map_train  = {k: img_map[k]   for k in train_keys_real}\n",
        "real_json_map_train = {k: json_map[k]  for k in train_keys_real}\n",
        "syn_img_map_train   = {k: syn_img_map[k] for k in train_keys_syn}\n",
        "\n",
        "real_img_map_val    = {k: img_map[k]   for k in val_keys}\n",
        "real_json_map_val   = {k: json_map[k]  for k in val_keys}\n",
        "syn_img_map_val     = {k: syn_img_map[k] for k in val_keys_syn}\n",
        "\n",
        "\n",
        "train_ds = TeethSegDataset(\n",
        "    real_img_map = real_img_map_train,\n",
        "    real_json_map= real_json_map_train,\n",
        "    syn_img_map  = syn_img_map_train,\n",
        "    syn_json_map = None,\n",
        "    mode         = DATA_MODE,\n",
        "    mix_strategy = MIX_STRATEGY,\n",
        "    syn_ratio    = SYN_RATIO,\n",
        "    transform    = train_tf\n",
        ")\n",
        "\n",
        "val_ds = TeethSegDataset(\n",
        "    real_img_map = real_img_map_val,\n",
        "    real_json_map= real_json_map_val,\n",
        "    syn_img_map  = syn_img_map_val,\n",
        "    syn_json_map = None,\n",
        "    mode         = DATA_MODE_VAL,\n",
        "    mix_strategy = MIX_STRATEGY_VAL,\n",
        "    syn_ratio    = SYN_RATIO,\n",
        "    transform    = val_tf\n",
        ")\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "_pin = (device.type == \"cuda\")\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=_pin, drop_last=True,\n",
        "                          persistent_workers=(NUM_WORKERS > 0))\n",
        "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=_pin,\n",
        "                          persistent_workers=(NUM_WORKERS > 0))\n",
        "\n",
        "\n",
        "NR_tr = len(train_keys_real)\n",
        "NS_tr_rep = len([k for k in train_keys_real if k in train_keys_syn])\n",
        "LEN_tr = len(train_ds)\n",
        "\n",
        "NR_val = len(val_keys)\n",
        "NS_val = len(val_keys_syn)\n",
        "LEN_val = len(val_ds)\n",
        "\n",
        "print(f\"Train (mix/replace): total={LEN_tr} | real_kept={NR_tr-NS_tr_rep} | synth_replaced={NS_tr_rep}\")\n",
        "print(f\"Val   (mix/concat) : total={LEN_val} | real={NR_val} | synth={NS_val}\")\n",
        "\n",
        "# Sanity check labels [0..32]\n",
        "xb, yb, *_ = next(iter(train_loader))\n",
        "print(\"xb:\", xb.shape, xb.dtype, float(xb.min()), float(xb.max()))\n",
        "print(\"yb:\", yb.shape, yb.dtype, int(yb.min()), int(yb.max()))\n",
        "print(\"yb unique (head):\", torch.unique(yb)[:20].tolist())\n",
        "\n",
        "assert yb.dtype == torch.long\n",
        "assert int(yb.min()) >= 0 and int(yb.max()) < N_CLASSES, \"Label out of range [0..32]\""
      ],
      "metadata": {
        "id": "OTMVANhu9k7R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beaf2261-7bde-4e4b-ca1e-3a5c45176022"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train (mix/replace): total=478 | real_kept=316 | synth_replaced=162\n",
            "Val   (mix/concat) : total=120 | real=120 | synth=38\n",
            "xb: torch.Size([8, 1, 256, 256]) torch.uint8 0.0 255.0\n",
            "yb: torch.Size([8, 256, 256]) torch.int64 0 32\n",
            "yb unique (head): [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dopo la costruzione di train_ds / val_ds\n",
        "assert len(syn_img_map) == len(syn_keys), \"Incoerenza syn map/keys.\"\n",
        "if DATA_MODE in (\"syn\",\"mix\"):\n",
        "    # pesca un paio di sample SYN e verifica che le mask non superino NUM_CLASSES-1\n",
        "    from collections import Counter\n",
        "    import numpy as np, random\n",
        "    probe = [k for k in syn_img_map.keys()]\n",
        "    for k in random.sample(probe, min(5, len(probe))):\n",
        "        img_p = syn_img_map[k]; json_p = json_map.get(k, None)\n",
        "        assert json_p is not None, f\"Manca JSON per chiave synth '{k}'\"\n"
      ],
      "metadata": {
        "id": "TqrJEVX51FVs"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First model: Mobile U-net\n",
        "\n",
        "class ConvBNReLU(nn.Module):\n",
        "    def __init__(self, c_in, c_out, k=3, s=1, p=1):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(c_in, c_out, k, s, p, bias=False),\n",
        "            nn.BatchNorm2d(c_out),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "    def forward(self, x): return self.block(x)\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, in_ch, embed_dim, patch_size=16):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.proj = nn.Conv2d(in_ch, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        B, D, H, W = x.shape\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        return x, (H, W)\n",
        "\n",
        "class PositionalEncoding2D(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super().__init__()\n",
        "        self.row_embed = nn.Parameter(torch.randn(1, 512, embed_dim // 2))\n",
        "        self.col_embed = nn.Parameter(torch.randn(1, 512, embed_dim // 2))\n",
        "    def forward(self, B, H, W, device, dtype):\n",
        "        row = self.row_embed[:, :H, :]\n",
        "        col = self.col_embed[:, :W, :]\n",
        "        pos = torch.cat([\n",
        "            row.unsqueeze(2).expand(1, H, W, -1),\n",
        "            col.unsqueeze(1).expand(1, H, W, -1)\n",
        "        ], dim=-1)\n",
        "        pos = pos.reshape(1, H*W, -1).to(device=device, dtype=dtype)\n",
        "        return pos.expand(B, -1, -1)\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim=256, depth=4, num_heads=8, mlp_ratio=4.0, drop=0.0):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleDict({\n",
        "                \"ln1\": nn.LayerNorm(embed_dim),\n",
        "                \"attn\": nn.MultiheadAttention(embed_dim, num_heads, dropout=drop, batch_first=True),\n",
        "                \"ln2\": nn.LayerNorm(embed_dim),\n",
        "                \"mlp\": nn.Sequential(\n",
        "                    nn.Linear(embed_dim, int(embed_dim*mlp_ratio)),\n",
        "                    nn.GELU(),\n",
        "                    nn.Dropout(drop),\n",
        "                    nn.Linear(int(embed_dim*mlp_ratio), embed_dim),\n",
        "                ),\n",
        "            }))\n",
        "    def forward(self, x):\n",
        "        for blk in self.layers:\n",
        "            h = blk[\"ln1\"](x)\n",
        "            attn_out, _ = blk[\"attn\"](h, h, h, need_weights=False)\n",
        "            x = x + attn_out\n",
        "            h = blk[\"ln2\"](x)\n",
        "            x = x + blk[\"mlp\"](h)\n",
        "        return x\n",
        "\n",
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, c_in, c_skip, c_out):\n",
        "        super().__init__()\n",
        "        self.conv1 = ConvBNReLU(c_in + c_skip, c_out)\n",
        "        self.conv2 = ConvBNReLU(c_out, c_out)\n",
        "    def forward(self, x, skip):\n",
        "        x = F.interpolate(x, size=skip.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
        "        x = torch.cat([x, skip], dim=1)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return x\n",
        "\n",
        "def _pad_to_multiple(x, mult=16):\n",
        "    B,C,H,W = x.shape\n",
        "    pad_h = (mult - H % mult) % mult\n",
        "    pad_w = (mult - W % mult) % mult\n",
        "    if pad_h or pad_w:\n",
        "        x = F.pad(x, (0, pad_w, 0, pad_h))\n",
        "    return x, (H, W), (pad_h, pad_w)\n",
        "\n",
        "def _unpad(x, orig_hw, pad_hw):\n",
        "    H0, W0 = orig_hw\n",
        "    if x.shape[-2:] != (H0, W0):\n",
        "        x = x[..., :H0, :W0]\n",
        "    return x\n",
        "\n",
        "# Second model: Trans U-net\n",
        "\n",
        "class TransUNet(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels=1, num_classes=2, base_ch=32, embed_dim=256,\n",
        "                 patch_size=16, depth=4, num_heads=8, mlp_ratio=4.0, drop=0.0):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # Encoder\n",
        "        self.enc1 = nn.Sequential(\n",
        "            ConvBNReLU(in_channels, base_ch),\n",
        "            ConvBNReLU(base_ch, base_ch),\n",
        "        )\n",
        "        self.down1 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.enc2 = nn.Sequential(\n",
        "            ConvBNReLU(base_ch, base_ch*2),\n",
        "            ConvBNReLU(base_ch*2, base_ch*2),\n",
        "        )\n",
        "        self.down2 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.enc3 = nn.Sequential(\n",
        "            ConvBNReLU(base_ch*2, base_ch*4),\n",
        "            ConvBNReLU(base_ch*4, base_ch*4),\n",
        "        )\n",
        "\n",
        "        self.conv_to_embed = nn.Conv2d(base_ch*4, embed_dim, kernel_size=1, bias=False)\n",
        "\n",
        "        self.patch_embed = PatchEmbed(embed_dim, embed_dim, patch_size=patch_size)\n",
        "        self.pos = PositionalEncoding2D(embed_dim)\n",
        "        self.tr = TransformerEncoder(embed_dim=embed_dim, depth=depth,\n",
        "                                     num_heads=num_heads, mlp_ratio=mlp_ratio, drop=drop)\n",
        "\n",
        "        # Decoder\n",
        "        self.dec3 = UpBlock(embed_dim, base_ch*4, base_ch*4)\n",
        "        self.dec2 = UpBlock(base_ch*4, base_ch*2, base_ch*2)\n",
        "        self.dec1 = UpBlock(base_ch*2, base_ch,   base_ch)\n",
        "\n",
        "        self.head = nn.Conv2d(base_ch, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, orig_hw, pad_hw = _pad_to_multiple(x, mult=self.patch_size)\n",
        "\n",
        "        # Encoder\n",
        "        s1 = self.enc1(x)\n",
        "        x  = self.down1(s1)\n",
        "        s2 = self.enc2(x)\n",
        "        x  = self.down2(s2)\n",
        "        s3 = self.enc3(x)\n",
        "\n",
        "        # to embedding space\n",
        "        feat = self.conv_to_embed(s3) # (B, D, H/4, W/4)\n",
        "\n",
        "        # patchify + positional + transformer\n",
        "        tokens, (h_p, w_p) = self.patch_embed(feat)\n",
        "        pos = self.pos(tokens.shape[0], h_p, w_p, tokens.device, tokens.dtype)\n",
        "        tokens = tokens + pos\n",
        "        tokens = self.tr(tokens)\n",
        "\n",
        "        # unpatchify\n",
        "        B, N, D = tokens.shape\n",
        "        feat_tr = tokens.transpose(1, 2).reshape(B, D, h_p, w_p)\n",
        "\n",
        "        # Decoder with skip\n",
        "        x = self.dec3(feat_tr, s3)\n",
        "        x = self.dec2(x,      s2)\n",
        "        x = self.dec1(x,      s1)\n",
        "        logits = self.head(x)\n",
        "        logits = _unpad(logits, orig_hw, pad_hw)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "VJ7Kot9J9lLw"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_class_weights(loader, n_classes, bg_weight=0.2, power=0.5):\n",
        "    hist = torch.zeros(n_classes, dtype=torch.long)\n",
        "    for _, y, _ in loader:\n",
        "        y = y.view(-1)\n",
        "        hist += torch.bincount(y, minlength=n_classes)\n",
        "    freq = (hist.float() / hist.sum().clamp_min(1)).clamp_min(1e-6)\n",
        "    w = (1.0 / freq)**power\n",
        "    w = w / w.mean()\n",
        "    w[0] = bg_weight\n",
        "    return w\n",
        "\n",
        "def make_warmup_cosine(optimizer, epochs, warmup_epochs=3):\n",
        "    def lr_lambda(e):\n",
        "        if e < warmup_epochs:\n",
        "            return (e + 1) / float(warmup_epochs)\n",
        "        t = (e - warmup_epochs) / max(1, (epochs - warmup_epochs))\n",
        "        return 0.5 * (1.0 + math.cos(math.pi * t))\n",
        "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "def _confmat_update(cm, preds, targets, n_classes):\n",
        "    preds   = preds.view(-1).to(torch.int64)\n",
        "    targets = targets.view(-1).to(torch.int64)\n",
        "    k = (targets >= 0) & (targets < n_classes)\n",
        "    inds = n_classes * targets[k] + preds[k]\n",
        "    cm += torch.bincount(inds, minlength=n_classes**2).reshape(n_classes, n_classes)\n",
        "    return cm\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_metrics(model, loader, criterion, device, n_classes):\n",
        "    model.eval()\n",
        "    val_loss_acc = 0.0\n",
        "    cm = torch.zeros((n_classes, n_classes), dtype=torch.long, device=device)\n",
        "    for imgs, masks, _ in loader:\n",
        "        imgs  = imgs.to(device, dtype=torch.float32)\n",
        "        masks = masks.to(device)\n",
        "        with torch.amp.autocast('cuda', enabled=(device.type=='cuda')):\n",
        "            logits = model(imgs)\n",
        "            val_loss_acc += criterion(logits, masks).item()\n",
        "        preds = logits.argmax(1)\n",
        "        cm = _confmat_update(cm, preds, masks, n_classes)\n",
        "\n",
        "    tp = cm.diag().to(torch.float32)\n",
        "    fp = cm.sum(0) - tp\n",
        "    fn = cm.sum(1) - tp\n",
        "    denom_iou  = (tp + fp + fn).clamp_min(1e-9)\n",
        "    denom_dice = (2*tp + fp + fn).clamp_min(1e-9)\n",
        "    iou_per_class  = (tp / denom_iou).detach().cpu().numpy()\n",
        "    dice_per_class = (2*tp / denom_dice).detach().cpu().numpy()\n",
        "    return {\n",
        "        \"val_loss\": val_loss_acc/len(loader),\n",
        "        \"mIoU\": float(iou_per_class.mean()),\n",
        "        \"mDice\": float(dice_per_class.mean()),\n",
        "        \"IoU_per_class\": iou_per_class,\n",
        "        \"Dice_per_class\": dice_per_class,\n",
        "    }\n",
        "\n",
        "\n",
        "def make_warmup_cosine(optimizer, epochs, warmup_epochs=3):\n",
        "    import math\n",
        "    def lr_lambda(e):\n",
        "        if e < warmup_epochs:\n",
        "            return (e + 1) / float(warmup_epochs)\n",
        "        t = (e - warmup_epochs) / max(1, (epochs - warmup_epochs))\n",
        "        return 0.5 * (1.0 + math.cos(math.pi * t))\n",
        "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "def train_one_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    criterion,\n",
        "    *,\n",
        "    name=\"model\",\n",
        "    epochs=10,\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    tmax=5,\n",
        "    patience=5,\n",
        "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    n_classes=None,\n",
        "\n",
        "    scheduler_kind: str = \"cosine\",\n",
        "    warmup_epochs: int = 3,\n",
        "    enable_clip: bool = False,\n",
        "    clip_max_norm: float = 1.0,\n",
        "):\n",
        "    assert n_classes is not None, \"Passa n_classes=N_CLASSES per calcolare le metriche.\"\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    if scheduler_kind == \"warmup_cosine\":\n",
        "        scheduler = make_warmup_cosine(optimizer, epochs=epochs, warmup_epochs=warmup_epochs)\n",
        "    else:\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=tmax)\n",
        "\n",
        "    use_amp = (device.type == \"cuda\")\n",
        "    scaler  = torch.amp.GradScaler('cuda', enabled=use_amp)\n",
        "\n",
        "    best_val = float(\"inf\"); best_epoch = -1; wait = 0\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        train_loss_acc = 0.0\n",
        "\n",
        "        for imgs, masks, _ in tqdm(train_loader, desc=f\"[{name}] Epoch {epoch}/{epochs}\", leave=False):\n",
        "            imgs  = imgs.to(device, dtype=torch.float32)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with torch.amp.autocast('cuda', enabled=use_amp):\n",
        "                logits = model(imgs)\n",
        "                loss   = criterion(logits, masks)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            if enable_clip:\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_max_norm)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            train_loss_acc += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if epoch % 3 == 0:\n",
        "            val_pack = compute_metrics(model, val_loader, criterion, device, n_classes)\n",
        "            val_loss = val_pack[\"val_loss\"]\n",
        "            print(f\"[{name}] Epoch {epoch} | Train L:{train_loss_acc/len(train_loader):.4f} | \"\n",
        "                  f\"Val L:{val_loss:.4f} | mIoU:{val_pack['mIoU']:.4f} | mDice:{val_pack['mDice']:.4f}\")\n",
        "\n",
        "            if val_loss < best_val:\n",
        "                best_val, best_epoch, wait = val_loss, epoch, 0\n",
        "            else:\n",
        "                wait += 1\n",
        "                if wait >= patience:\n",
        "                    print(f\"⏹️  [{name}] Early stopping @ epoch {epoch} (best {best_epoch})\")\n",
        "                    break\n",
        "\n",
        "    final_metrics = compute_metrics(model, val_loader, criterion, device, n_classes)\n",
        "    print(f\"🧩 [{name}] Best @ epoch {best_epoch} | best_val_loss={best_val:.4f} | \"\n",
        "          f\"mIoU={final_metrics['mIoU']:.4f} | mDice={final_metrics['mDice']:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"name\": name,\n",
        "        \"best_epoch\": best_epoch,\n",
        "        \"best_val_loss\": best_val,\n",
        "        \"metrics\": final_metrics,\n",
        "    }\n",
        "\n",
        "#  TransUNet training parameters\n",
        "LR_TR  = 5e-4\n",
        "WD_TR  = 5e-2\n",
        "CLIP_N = 1.0\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Loss\n",
        "class_w = compute_class_weights(train_loader, N_CLASSES)\n",
        "class_w = class_w.to(device)\n",
        "\n",
        "# New loss: weighted CE + smoothing + Dice\n",
        "ce_loss   = nn.CrossEntropyLoss(weight=class_w, label_smoothing=0.05)\n",
        "dice_loss = smp.losses.DiceLoss(mode=\"multiclass\")\n",
        "def criterion(logits, targets):\n",
        "    return ce_loss(logits, targets) + dice_loss(logits, targets)\n",
        "\n",
        "\n",
        "# Training\n",
        "best_val, best_epoch, wait, patience = float(\"inf\"), -1, 0, 2"
      ],
      "metadata": {
        "id": "Ix6sLH7392cJ"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#U-net\n",
        "cnn = smp.Unet(\"mobilenet_v2\", encoder_weights=\"imagenet\", in_channels=1, classes=N_CLASSES).to(device)\n",
        "results_cnn = train_one_model(cnn, train_loader, val_loader, criterion,\n",
        "                          name=\"unet_mobilenetv2\", epochs=EPOCHS, n_classes=N_CLASSES)\n",
        "#Trans U-net\n",
        "tr = TransUNet(\n",
        "    in_channels=1,\n",
        "    num_classes=N_CLASSES,\n",
        "    base_ch=32,\n",
        "    embed_dim=256,\n",
        "    patch_size=16,\n",
        "    depth=6,\n",
        "    num_heads=8,\n",
        "    mlp_ratio=4.0,\n",
        "    drop=0.1\n",
        ").to(device)\n",
        "\n",
        "\n",
        "results_tr = train_one_model(\n",
        "    tr, train_loader, val_loader, criterion,\n",
        "    name=\"transunet\",\n",
        "    epochs=EPOCHS, n_classes=N_CLASSES,\n",
        "    lr=5e-4, weight_decay=5e-2,\n",
        "    scheduler_kind=\"warmup_cosine\",\n",
        "    warmup_epochs=3,\n",
        "    enable_clip=True, clip_max_norm=1.0\n",
        ")"
      ],
      "metadata": {
        "id": "zQWHQxQR98_h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17c364a8-ce9f-46ef-da3a-91d348290dd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[unet_mobilenetv2] Epoch 3 | Train L:2.7758 | Val L:2.6643 | mIoU:0.1795 | mDice:0.2838\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[unet_mobilenetv2] Epoch 6 | Train L:2.5713 | Val L:2.5343 | mIoU:0.2330 | mDice:0.3611\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[unet_mobilenetv2] Epoch 9 | Train L:2.5893 | Val L:2.8103 | mIoU:0.1242 | mDice:0.1966\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[unet_mobilenetv2] Epoch 12 | Train L:2.4278 | Val L:2.3783 | mIoU:0.2757 | mDice:0.4065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[unet_mobilenetv2] Epoch 15 | Train L:2.1056 | Val L:2.0381 | mIoU:0.5129 | mDice:0.6714\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[unet_mobilenetv2] Epoch 17/100:  39%|███▉      | 23/59 [00:03<00:05,  6.14it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "TXmJiyV564lD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_predictions(\n",
        "    model,\n",
        "    loader,\n",
        "    palette,\n",
        "    out_dir,\n",
        "    device,\n",
        "    use_amp=True,\n",
        "    suffix=\"pred\"\n",
        "):\n",
        "\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for imgs, masks, keys in tqdm(loader, desc=f\"Saving predictions [{suffix}]\"):\n",
        "            imgs = imgs.to(device, dtype=torch.float32)\n",
        "            with torch.amp.autocast('cuda', enabled=(device.type==\"cuda\" and use_amp)):\n",
        "                logits = model(imgs)\n",
        "                pred_idx = torch.softmax(logits, 1).argmax(1)\n",
        "\n",
        "            pred_idx = pred_idx.cpu().numpy().astype(np.uint8)\n",
        "            for i, k in enumerate(keys):\n",
        "                color_mask = palette[pred_idx[i]]\n",
        "                cv2.imwrite(os.path.join(out_dir, f\"{k}_{suffix}_color.png\"),\n",
        "                            cv2.cvtColor(color_mask, cv2.COLOR_RGB2BGR))\n",
        "                cv2.imwrite(os.path.join(out_dir, f\"{k}_{suffix}_idx.png\"), pred_idx[i])"
      ],
      "metadata": {
        "id": "S4sxjJhZZ02O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_predictions(\n",
        "    cnn, val_loader, palette,\n",
        "    out_dir=os.path.join(ROOT, \"predictions_unet\"),\n",
        "    device=device,\n",
        "    suffix=\"unet\"\n",
        ")\n",
        "\n",
        "save_predictions(\n",
        "    tr, val_loader, palette,\n",
        "    out_dir=os.path.join(ROOT, \"predictions_transunet\"),\n",
        "    device=device,\n",
        "    suffix=\"transunet\"\n",
        ")"
      ],
      "metadata": {
        "id": "AuJtzQpbfnG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def visualize_models_comparison_rows(\n",
        "    model_A_name, model_A,\n",
        "    model_B_name, model_B,\n",
        "    loader,\n",
        "    *,\n",
        "    max_rows=6\n",
        "):\n",
        "\n",
        "    device = next(model_A.parameters()).device\n",
        "    use_amp = (device.type == \"cuda\")\n",
        "\n",
        "    def colorize_idx(idx_map):\n",
        "        vis = idx_map.astype(np.int32).copy()\n",
        "        if 'IGNORE_INDEX' in globals() and IGNORE_INDEX is not None:\n",
        "            vis[vis == IGNORE_INDEX] = 0\n",
        "        return palette[vis]\n",
        "\n",
        "    def to_vis_gray(t: torch.Tensor) -> np.ndarray:\n",
        "        arr = t.detach().cpu().float()\n",
        "        if arr.ndim == 3 and arr.shape[0] == 1:\n",
        "            arr = arr[0]\n",
        "        mn, mx = float(arr.min()), float(arr.max())\n",
        "        if mx > mn:\n",
        "            arr = (arr - mn) / (mx - mn)\n",
        "        else:\n",
        "            arr = torch.zeros_like(arr)\n",
        "        return arr.numpy()\n",
        "\n",
        "    def sample_metrics(gt_np, pr_np):\n",
        "        ious, dices = [], []\n",
        "        for c in range(1, N_CLASSES):\n",
        "            gt_c = (gt_np == c); pr_c = (pr_np == c)\n",
        "            gs, ps = int(gt_c.sum()), int(pr_c.sum())\n",
        "            if gs == 0 and ps == 0:\n",
        "                continue\n",
        "            inter = int((gt_c & pr_c).sum())\n",
        "            union = gs + ps - inter\n",
        "            ious.append((inter+1e-6)/(union+1e-6))\n",
        "            dices.append((2*inter+1e-6)/(gs+ps+1e-6))\n",
        "        miou  = float(np.mean(ious))  if ious  else 0.0\n",
        "        mdice = float(np.mean(dices)) if dices else 0.0\n",
        "        return mdice, miou\n",
        "\n",
        "    model_A.eval(); model_B.eval()\n",
        "    shown = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, masks, keys in loader:\n",
        "            if shown >= max_rows: break\n",
        "            imgs  = imgs.to(device, dtype=torch.float32)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            with torch.amp.autocast('cuda', enabled=use_amp):\n",
        "                preds_A = torch.softmax(model_A(imgs), 1).argmax(1)\n",
        "                preds_B = torch.softmax(model_B(imgs), 1).argmax(1)\n",
        "\n",
        "            for i in range(imgs.size(0)):\n",
        "                if shown >= max_rows: break\n",
        "\n",
        "                img_vis = to_vis_gray(imgs[i])\n",
        "                gt_np   = masks[i].cpu().numpy()\n",
        "                prA_np  = preds_A[i].cpu().numpy()\n",
        "                prB_np  = preds_B[i].cpu().numpy()\n",
        "\n",
        "                mdice_A, miou_A = sample_metrics(gt_np, prA_np)\n",
        "                mdice_B, miou_B = sample_metrics(gt_np, prB_np)\n",
        "\n",
        "                gt_rgb  = colorize_idx(gt_np)\n",
        "                prA_rgb = colorize_idx(prA_np)\n",
        "                prB_rgb = colorize_idx(prB_np)\n",
        "\n",
        "                fig, axes = plt.subplots(1, 6, figsize=(20, 4))\n",
        "                title = (f\"{keys[i]}  |  \"\n",
        "                         f\"[{model_A_name}] mDice:{mdice_A:.3f} mIoU:{miou_A:.3f}   |   \"\n",
        "                         f\"[{model_B_name}] mDice:{mdice_B:.3f} mIoU:{miou_B:.3f}\")\n",
        "                fig.suptitle(title, y=0.94, fontsize=11)\n",
        "\n",
        "                # columns\n",
        "                axes[0].imshow(img_vis, cmap=\"gray\"); axes[0].set_title(\"X-ray\", fontsize=9); axes[0].axis(\"off\")\n",
        "                axes[1].imshow(gt_rgb); axes[1].set_title(\"Mask (GT)\", fontsize=9); axes[1].axis(\"off\")\n",
        "                axes[2].imshow(prA_rgb); axes[2].set_title(f\"{model_A_name} (pred)\", fontsize=9); axes[2].axis(\"off\")\n",
        "                axes[3].imshow(prB_rgb); axes[3].set_title(f\"{model_B_name} (pred)\", fontsize=9); axes[3].axis(\"off\")\n",
        "                axes[4].imshow(img_vis, cmap=\"gray\"); axes[4].imshow(prA_rgb, alpha=0.35)\n",
        "                axes[4].set_title(f\"Overlay {model_A_name}\", fontsize=9); axes[4].axis(\"off\")\n",
        "                axes[5].imshow(img_vis, cmap=\"gray\"); axes[5].imshow(prB_rgb, alpha=0.35)\n",
        "                axes[5].set_title(f\"Overlay {model_B_name}\", fontsize=9); axes[5].axis(\"off\")\n",
        "\n",
        "                plt.subplots_adjust(top=0.82, wspace=0.1)\n",
        "                fig.subplots_adjust(bottom=0.05)\n",
        "                fig.add_artist(plt.Line2D([0, 1], [0.02, 0.02], color=\"black\", linewidth=1, transform=fig.transFigure))\n",
        "\n",
        "                plt.show()\n",
        "                shown += 1"
      ],
      "metadata": {
        "id": "nWKpu6BHm0DD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_models_comparison_rows(\n",
        "    model_A_name=\"TransUNet \", model_A=tr,\n",
        "    model_B_name=\"U-Net MobileNetV2\", model_B=cnn,\n",
        "    loader=val_loader,\n",
        "    max_rows=6\n",
        ")"
      ],
      "metadata": {
        "id": "EuhTmz4VulTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Results in a .txt file\n",
        "import datetime\n",
        "\n",
        "def _maybe(n,d=None): return globals().get(n,d)\n",
        "def _fmt(x, nd=4): return \"N/A\" if x is None else (f\"{x:.{nd}f}\" if isinstance(x,(int,float)) else str(x))\n",
        "\n",
        "def _extract_or_compute(run, model, name):\n",
        "    if isinstance(run, dict) and \"metrics\" in run:\n",
        "        m = run[\"metrics\"]\n",
        "        return {\n",
        "            \"name\": name,\n",
        "            \"best_epoch\": run.get(\"best_epoch\"),\n",
        "            \"best_val_loss\": run.get(\"best_val_loss\"),\n",
        "            \"mIoU\": m.get(\"mIoU\"),\n",
        "            \"mDice\": m.get(\"mDice\"),\n",
        "            \"IoU_per_class\": m.get(\"IoU_per_class\"),\n",
        "            \"Dice_per_class\": m.get(\"Dice_per_class\"),\n",
        "        }\n",
        "\n",
        "    cmf = _maybe(\"compute_metrics\")\n",
        "    if (model is not None) and (cmf is not None) and _maybe(\"val_loader\") is not None:\n",
        "        met = cmf(model, _maybe(\"val_loader\"), _maybe(\"criterion\"), _maybe(\"device\"), _maybe(\"N_CLASSES\"))\n",
        "        return {\n",
        "            \"name\": name,\n",
        "            \"best_epoch\": None,\n",
        "            \"best_val_loss\": float(met.get(\"val_loss\")),\n",
        "            \"mIoU\": float(met.get(\"mIoU\")),\n",
        "            \"mDice\": float(met.get(\"mDice\")),\n",
        "            \"IoU_per_class\": met.get(\"IoU_per_class\"),\n",
        "            \"Dice_per_class\": met.get(\"Dice_per_class\"),\n",
        "        }\n",
        "    return None\n",
        "\n",
        "def _fmt_table(iou_arr, dice_arr, class_titles=None):\n",
        "    if iou_arr is None or dice_arr is None: return [\"  (per-class metrics not available)\"]\n",
        "    K = max(len(iou_arr), len(dice_arr))\n",
        "    lines = [\"  idx  title  IoU      Dice\",\"  ---- ------ -------- --------\"]\n",
        "    for c in range(K):\n",
        "        title = \"bg\" if c==0 else (str(class_titles[c-1]) if class_titles and 0 <= c-1 < len(class_titles) else \"-\")\n",
        "        iou  = _fmt(iou_arr[c])  if c < len(iou_arr)  else \"N/A\"\n",
        "        dice = _fmt(dice_arr[c]) if c < len(dice_arr) else \"N/A\"\n",
        "        lines.append(f\"  {c:>3}  {title:>5}  {iou:>8} {dice:>8}\")\n",
        "    return lines\n",
        "\n",
        "# dataset and setup\n",
        "CLASS_TITLES = _maybe(\"CLASS_TITLES\", [str(i) for i in range(1, _maybe(\"N_CLASSES\",1))])\n",
        "setup_lines = []\n",
        "setup_lines.append(\"DATASET\")\n",
        "setup_lines.append(f\"  IMG_SIZE        : {_fmt(_maybe('IMG_SIZE'))}\")\n",
        "setup_lines.append(f\"  N_CLASSES       : {_fmt(_maybe('N_CLASSES'))}\")\n",
        "setup_lines.append(f\"  SPLIT (val)     : {_fmt(_maybe('val_ratio'))}\")\n",
        "setup_lines.append(f\"  TRAIN  real/syn : {len(_maybe('real_img_map_train',{}))} / {len(_maybe('syn_img_map_train',{}))}\")\n",
        "setup_lines.append(f\"  VAL    real/syn : {len(_maybe('real_img_map_val',{}))} / {len(_maybe('syn_img_map_val',{}))}\")\n",
        "setup_lines.append(\"\")\n",
        "setup_lines.append(\"SETUP\")\n",
        "setup_lines.append(f\"  BATCH_SIZE      : {_fmt(_maybe('BATCH_SIZE'))}\")\n",
        "setup_lines.append(f\"  NUM_WORKERS     : {_fmt(_maybe('NUM_WORKERS'))}\")\n",
        "setup_lines.append(f\"  EPOCHS          : {_fmt(_maybe('EPOCHS'))}\")\n",
        "lr = _maybe('LR'); wd = _maybe('WEIGHT_DECAY')\n",
        "if lr is None and 'optimizer' in globals(): lr = globals()['optimizer'].param_groups[0].get('lr')\n",
        "if wd is None and 'optimizer' in globals(): wd = globals()['optimizer'].param_groups[0].get('weight_decay')\n",
        "tmax = _maybe('T_MAX');\n",
        "if tmax is None and _maybe('scheduler') is not None: tmax = getattr(_maybe('scheduler'), 'T_max', None)\n",
        "setup_lines.append(f\"  LR / WD / T_MAX : {_fmt(lr)} / {_fmt(wd)} / {_fmt(tmax)}\")\n",
        "setup_lines.append(f\"  AMP             : {_fmt(_maybe('use_amp'))}\")\n",
        "setup_lines.append(f\"  DATA_MODE       : {_fmt(_maybe('DATA_MODE'))}  |  MIX_STRATEGY: {_fmt(_maybe('MIX_STRATEGY'))}\")\n",
        "setup_lines.append(f\"  DATA_MODE_VAL   : {_fmt(_maybe('DATA_MODE_VAL'))}  |  MIX_STRATEGY_VAL: {_fmt(_maybe('MIX_STRATEGY_VAL'))}\")\n",
        "setup_lines.append(f\"  SYN_RATIO       : {_fmt(_maybe('SYN_RATIO'))}\")\n",
        "\n",
        "\n",
        "unet  = _extract_or_compute(_maybe(\"results_cnn\"), _maybe(\"cnn\"), \"unet_mobilenetv2\")\n",
        "trans = _extract_or_compute(_maybe(\"results_tr\"),  _maybe(\"tr\"),  \"transunet\")\n",
        "\n",
        "def _score(d):\n",
        "    if not d: return -1.0\n",
        "    return (d.get(\"mDice\") if d.get(\"mDice\") is not None else d.get(\"mIoU\", -1.0)) or -1.0\n",
        "\n",
        "best = max([x for x in [unet, trans] if x], key=_score, default=None)\n",
        "\n",
        "\n",
        "now = datetime.datetime.now()\n",
        "fname = f\"RESULTS_{now.strftime('%Y%m%d_%H%M')}_{DATA_MODE}.txt\"\n",
        "\n",
        "lines = [f\"RESULTS — {now.strftime('%Y-%m-%d %H:%M:%S')}\",\n",
        "         \"=\"*64] + setup_lines + [\"-\"]\n",
        "\n",
        "def _block(d, title):\n",
        "    if not d: return [f\"{title}: N/A\"]\n",
        "    out = [\n",
        "        f\"{title}\",\n",
        "        f\"  best_epoch     : {_fmt(d['best_epoch'])}\",\n",
        "        f\"  best_val_loss  : {_fmt(d['best_val_loss'])}\",\n",
        "        f\"  mIoU           : {_fmt(d['mIoU'])}\",\n",
        "        f\"  mDice          : {_fmt(d['mDice'])}\",\n",
        "        f\"  per-class:\",\n",
        "    ]\n",
        "    out += _fmt_table(d.get(\"IoU_per_class\"), d.get(\"Dice_per_class\"), CLASS_TITLES)\n",
        "    return out\n",
        "\n",
        "lines += _block(unet, \"UNet\")\n",
        "lines.append(\"-\")\n",
        "lines += _block(trans, \"TransUNet\")\n",
        "lines.append(\"-\")\n",
        "lines.append(f\"BEST (by mDice→mIoU): {best['name']} — mDice={_fmt(best.get('mDice'))}  mIoU={_fmt(best.get('mIoU'))}\" if best else \"BEST: N/A\")\n",
        "lines.append(\"=\"*64)\n",
        "\n",
        "with open(fname, \"w\") as f:\n",
        "    f.write(\"\\n\".join(lines))\n",
        "\n",
        "print(f\"✅ Saved {fname}\\n\")\n",
        "print(\"\\n\".join(lines[:16]), \"\\n...\\n\", \"\\n\".join(lines[-8:]), sep=\"\")"
      ],
      "metadata": {
        "id": "XLFA0IWbs1kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JYdkT5BiyfyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4RbQEcVtye87"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}